"""
ETL Pipeline - FIFA World Cup Matches (1930-2014)
Pipeline complet : Extract -> Transform -> Load
Auteurs : √âquipe Data Engineering
Date : D√©cembre 2024
"""
import json
import pandas as pd
import sqlite3
import re
from datetime import datetime
from unidecode import unidecode
import logging
from pathlib import Path

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# =====================================================================
# DICTIONNAIRES DE MAPPING (Data Engineer 2)
# =====================================================================

TEAMS_MAPPING = {
    # Allemagne
    "West Germany": "Germany",
    "FR Germany": "Germany",
    "German DR": "East Germany",
    "Germany FR": "Germany",
    
    # URSS / Russie
    "Soviet Union": "Russia",
    "USSR": "Russia",
    
    # Yougoslavie
    "Yugoslavia": "Serbia",
    
    # Tch√©coslovaquie
    "Czechoslovakia": "Czech Republic",
    
    # Cor√©e
    "Korea Republic": "South Korea",
    "Korea DPR": "North Korea",
    "South Korea": "South Korea",
    
    # USA
    "United States": "USA",
    "US": "USA",
    
    # Autres variations
    "Ivory Coast": "Cote d'Ivoire",
    "C√¥te d'Ivoire": "Cote d'Ivoire",
    "Bosnia-Herzegovina": "Bosnia and Herzegovina",
    "Iran": "Iran",
    "Irish Republic": "Republic of Ireland",
    "Northern Ireland": "Northern Ireland",
    
    # Casse (sera g√©r√© automatiquement mais on garde pour historique)
    "FRANCE": "France",
    "BRAZIL": "Brazil",
    "ARGENTINA": "Argentina",
}

CITIES_MAPPING = {
    "MONTEVIDEO": "Montevideo",
    "MEXICO CITY": "Mexico City",
    "Mexico (M√©xico)": "Mexico City",
    "M√©xico": "Mexico City",
    "Sao Paulo": "S√£o Paulo",
    "S√£o Paulo": "S√£o Paulo",
    "Rio de Janeiro": "Rio de Janeiro",
    "Saint-Denis": "Paris",
    "Saint Denis": "Paris",
    "ROME": "Rome",
    "PARIS": "Paris",
    "BERLIN": "Berlin",
    "LONDON": "London",
    "MADRID": "Madrid",
    "BARCELONA": "Barcelona",
    "BUENOS AIRES": "Buenos Aires",
    "Brasilia": "Bras√≠lia",
    "Bras√≠lia": "Bras√≠lia",
    "Belo Horizonte": "Belo Horizonte",
    "Porto Alegre": "Porto Alegre",
    "Curitiba": "Curitiba",
    "Manaus": "Manaus",
    "Fortaleza": "Fortaleza",
    "Recife": "Recife",
    "Salvador": "Salvador",
    "Natal": "Natal",
    "Cuiaba": "Cuiab√°",
}

ROUNDS_MAPPING = {
    # Group Stage
    "GROUP_STAGE": "Group Stage",
    "Group A": "Group Stage",
    "Group B": "Group Stage",
    "Group C": "Group Stage",
    "Group D": "Group Stage",
    "Group E": "Group Stage",
    "Group F": "Group Stage",
    "Group G": "Group Stage",
    "Group H": "Group Stage",
    "Group 1": "Group Stage",
    "Group 2": "Group Stage",
    "Group 3": "Group Stage",
    "Group 4": "Group Stage",
    "Poules": "Group Stage",
    "First round": "Group Stage",
    "Preliminary round": "Group Stage",
    
    # Round of 16
    "8e de finale": "Round of 16",
    "Round of 16": "Round of 16",
    "ROUND_OF_16": "Round of 16",
    "Eighth-finals": "Round of 16",
    
    # Quarter-finals
    "1/4 finale": "Quarter-finals",
    "Quarter-finals": "Quarter-finals",
    "QUARTER_FINALS": "Quarter-finals",
    "Quarterfinals": "Quarter-finals",
    
    # Semi-finals
    "1/2 finale": "Semi-finals",
    "Semi-finals": "Semi-finals",
    "SEMI_FINALS": "Semi-finals",
    "Semifinals": "Semi-finals",
    
    # Third Place
    "3rd place": "Third Place",
    "Match pour la 3e place": "Third Place",
    "Third place": "Third Place",
    "Play-off for third place": "Third Place",
    
    # Final
    "Final": "Final",
    "Finale": "Final",
    "FINAL": "Final",
}

TEAMS_MAPPING_2018 = {
    1: "Russia",
    2: "Saudi Arabia",
    3: "Egypt",
    4: "Uruguay",
    5: "Portugal",
    6: "Spain",
    7: "Morocco",
    8: "Iran",
    9: "France",
    10: "Australia",
    11: "Peru",
    12: "Denmark",
    13: "Argentina",
    14: "Iceland",
    15: "Croatia",
    16: "Nigeria",
    17: "Brazil",
    18: "Switzerland",
    19: "Costa Rica",
    20: "Serbia",
    21: "Germany",
    22: "Mexico",
    23: "Sweden",
    24: "South Korea",
    25: "Belgium",
    26: "Panama",
    27: "Tunisia",
    28: "England",
    29: "Poland",
    30: "Senegal",
    31: "Colombia",
    32: "Japan"
}

# Mapping des stades
STADIUMS_MAPPING_2018 = {
    1: "Luzhniki Stadium",
    2: "Otkrytiye Arena",
    3: "Krestovsky Stadium",
    4: "Kaliningrad Stadium",
    5: "Kazan Arena",
    6: "Nizhny Novgorod Stadium",
    7: "Cosmos Arena",
    8: "Volgograd Arena",
    9: "Mordovia Arena",
    10: "Rostov Arena",
    11: "Fisht Olympic Stadium",
    12: "Central Stadium"
}

# =====================================================================
# CLASSE EXTRACT (Data Engineer 1)
# =====================================================================

class WorldCupExtractor:
    """Extraction des donn√©es depuis les 3 sources CSV"""
    
    def __init__(self, data_dir="data/raw"):
        self.data_dir = Path(data_dir)
        
    def extract_source1(self, filename="matches_1930-2010.csv"):
        """
        Extract matches_1930-2010.csv
        Format: edition, round, score, team1, team2, url, venue, year
        """
        logger.info(f"üì• Extraction de {filename}...")
        try:
            filepath = self.data_dir / filename
            df = pd.read_csv(filepath)
            logger.info(f"‚úÖ {len(df)} matchs extraits de {filename}")
            logger.debug(f"Colonnes: {list(df.columns)}")
            return df
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction {filename}: {e}")
            raise
    
    def extract_source2(self, filename="WorldCupMatches2014.csv"):
        """
        Extract WorldCupMatches2014.csv
        Votre fichier semble avoir un probl√®me de d√©limitation
        """
        logger.info(f"üì• Extraction de {filename}...")
        try:
            filepath = self.data_dir / filename
            
            # D'abord, regarder le contenu brut
            with open(filepath, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                second_line = f.readline().strip()
            
            logger.debug(f"Premi√®re ligne: {first_line[:100]}...")
            logger.debug(f"Deuxi√®me ligne: {second_line[:100]}...")
            
            # V√©rifier le s√©parateur
            if ';' in first_line and ',' not in first_line.replace('","', ''):
                sep = ';'
                logger.info(f"üîß S√©parateur d√©tect√©: point-virgule (;)")
            else:
                sep = ','
                logger.info(f"üîß S√©parateur d√©tect√©: virgule (,)")
            
            # Essayer de lire avec pandas
            df = pd.read_csv(filepath, sep=sep, encoding='utf-8')
            
            # Si on a toujours un probl√®me (toutes les donn√©es dans une colonne)
            if len(df.columns) == 1:
                logger.warning("‚ö†Ô∏è  Toutes les donn√©es dans une colonne, tentative de correction...")
                
                # Lire le fichier ligne par ligne
                with open(filepath, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # Analyser l'en-t√™te
                header = lines[0].strip().split(sep)
                logger.debug(f"Header splitt√©: {header}")
                
                # Nettoyer les guillemets
                header = [col.strip().replace('"', '') for col in header]
                
                # Lire les donn√©es
                data = []
                for line in lines[1:]:
                    row = line.strip().split(sep)
                    # Nettoyer les guillemets
                    row = [cell.strip().replace('"', '') for cell in row]
                    data.append(row)
                
                # Cr√©er DataFrame
                df = pd.DataFrame(data, columns=header)
            
            logger.info(f"‚úÖ {len(df)} matchs extraits de {filename}")
            logger.debug(f"Colonnes: {list(df.columns)}")
            logger.debug(f"Aper√ßu des premi√®res lignes:\n{df.head(3)}")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction {filename}: {e}")
            logger.error("Veuillez v√©rifier le format de votre fichier CSV")
            raise
    
    def extract_source3(self, filename="Fifa_world_cup_matches.csv"):
        """
        Extract Fifa_world_cup_matches.csv
        Format suppos√©: Contient plusieurs √©ditions
        """
        logger.info(f"üì• Extraction de {filename}...")
        try:
            filepath = self.data_dir / filename
            
            # Lire le fichier avec diff√©rents s√©parateurs possibles
            try:
                # Essayer d'abord avec la virgule
                df = pd.read_csv(filepath, encoding='utf-8')
                logger.info("üîß S√©parateur d√©tect√©: virgule (,)")
            except Exception:
                # Si √©chec, essayer point-virgule
                df = pd.read_csv(filepath, sep=';', encoding='utf-8')
                logger.info("üîß S√©parateur d√©tect√©: point-virgule (;)")
            
            # Nettoyer les noms de colonnes
            df.columns = [col.strip() for col in df.columns]
            
            logger.info(f"‚úÖ {len(df)} matchs extraits de {filename}")
            logger.info(f"üìä Colonnes: {list(df.columns)}")
            logger.debug(f"Aper√ßu:\n{df.head(3)}")
            
            return df
            
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction {filename}: {e}")
            raise

    def extract_source4(self, filename="data_2018.json"):
        """
        Extract data_2018.json - Coupe du Monde 2018
        """
        logger.info(f"üì• Extraction de {filename}...")
        try:
            filepath = self.data_dir / filename
            
            # Charger le JSON
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"‚úÖ JSON 2018 charg√© avec succ√®s")
            logger.info(f"üìä Structure: {list(data.keys())}")
            
            # Afficher les statistiques
            logger.info(f"""
    üìä Statistiques source 4 (2018):
    - Stades: {len(data.get('stadiums', []))}
    - √âquipes: {len(data.get('teams', []))}
    - Groupes: {len(data.get('groups', {}))}
    - Matchs de groupe: {sum(len(group['matches']) for group in data.get('groups', {}).values())}
    - Matchs √† √©limination directe: {len(data.get('knockout', {}).get('round_16', {}).get('matches', [])) +
                                    len(data.get('knockout', {}).get('round_8', {}).get('matches', [])) +
                                    len(data.get('knockout', {}).get('round_4', {}).get('matches', [])) +
                                    len(data.get('knockout', {}).get('round_2_loser', {}).get('matches', [])) +
                                    len(data.get('knockout', {}).get('round_2', {}).get('matches', []))}
            """)
            
            return data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur extraction {filename}: {e}")
            raise        
    # =====================================================================
# CLASSE TRANSFORM (Data Engineer 2)
# =====================================================================

class WorldCupTransformer:
    """Transformation et nettoyage des donn√©es"""
    
    def __init__(self):
        self.teams_mapping = TEAMS_MAPPING
        self.cities_mapping = CITIES_MAPPING
        self.rounds_mapping = ROUNDS_MAPPING
        self.stadiums_mapping = STADIUMS_MAPPING_2018
        self.teams_mapping_2018 = TEAMS_MAPPING_2018
    
    # -------------------- Fonctions utilitaires --------------------
    
    @staticmethod

    def parse_score(score_str):
        """
        Parse le score au format '4-1 (3-0)' ou '2-2'
        Returns: (home_goals, away_goals) ou (0, 0) si erreur
        """
        if pd.isna(score_str) or score_str is None:
            return 0, 0
        
        try:
            # Convertir en string et nettoyer
            score_clean = str(score_str).strip()
            
            # Regex pour capturer X-Y au d√©but
            match = re.match(r'(\d+)-(\d+)', score_clean)
            if match:
                home = int(match.group(1))
                away = int(match.group(2))
                return max(0, home), max(0, away)  # S'assurer que c'est >= 0
            return 0, 0
        except:
            return 0, 0
    
    def normalize_team(self, team_name):
        """Normalise le nom d'une √©quipe"""
        if pd.isna(team_name):
            return None
        
        team = str(team_name).strip()
        
        # Nettoyer les guillemets et espaces
        team = team.replace('"', '').strip()
        
        # Mapping historique
        if team in self.teams_mapping:
            team = self.teams_mapping[team]
        
        # Normalisation casse (Title Case)
        team = team.title()
        
        # Suppression accents optionnelle (d√©commenter si souhait√©)
        # team = unidecode(team)
        
        return team
    
    def normalize_city(self, city_name):
        """Normalise le nom d'une ville"""
        if pd.isna(city_name):
            return None
        
        city = str(city_name).strip()
        
        # Nettoyage guillemets
        city = city.replace('"', '')
        
        # Nettoyage parenth√®ses
        city = re.sub(r'\([^)]*\)', '', city).strip()
        
        # Mapping manuel
        if city in self.cities_mapping:
            city = self.cities_mapping[city]
        
        # Title Case si pas dans mapping
        city = city.title()
        
        return city
    
    def normalize_round(self, round_str):
        """Normalise le nom du tour"""
        if pd.isna(round_str):
            return None
        
        round_clean = str(round_str).strip()
        
        # Nettoyage guillemets
        round_clean = round_clean.replace('"', '')
        
        # Mapping exact
        if round_clean in self.rounds_mapping:
            return self.rounds_mapping[round_clean]
        
        # D√©tection groupe (Group X, Poule X)
        if 'group' in round_clean.lower() or 'poule' in round_clean.lower():
            return "Group Stage"
        
        # Si rien trouv√©, Title Case
        return round_clean.title()
    
    @staticmethod
    def compute_result(home_goals, away_goals, home_team=None, away_team=None):
        """Calcule le r√©sultat du match avec nom du gagnant"""
        if pd.isna(home_goals) or pd.isna(away_goals):
            return None
        
        if home_goals > away_goals:
            if home_team:
                return f"{home_team}"  # Nom de l'√©quipe √† domicile
            else:
                return "home_team"
        elif away_goals > home_goals:
            if away_team:
                return f"{away_team}"  # Nom de l'√©quipe √† l'ext√©rieur
            else:
                return "away_team"
        else:
            return "draw"  # Match nul
    
    @staticmethod
    def parse_datetime(datetime_str):
        """
        Parse diverses formats de dates
        Exemples: 
        - '12 Jun 2014 - 17:00'
        - '1930-07-13'
        - '13 Jul 1930'
        """
        if pd.isna(datetime_str):
            return None
        
        datetime_str = str(datetime_str).strip().replace('"', '')
        
        try:
            # Format: '12 Jun 2014 - 17:00'
            if ' - ' in datetime_str:
                date_part = datetime_str.split(' - ')[0].strip()
                return pd.to_datetime(date_part, format='%d %b %Y')
            
            # Format: '13 Jul 1930'
            if len(datetime_str.split()) == 3:
                return pd.to_datetime(datetime_str, format='%d %b %Y')
            
            # Format ISO
            return pd.to_datetime(datetime_str)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Date non parsable '{datetime_str}': {e}")
            return None
    
    # -------------------- Transformations par source --------------------
    
    def transform_source1(self, df):
        """
        Transformation matches_1930-2010.csv
        """
        logger.info("üîÑ Transformation Source 1 (1930-2010)...")
        logger.debug(f"Colonnes re√ßues: {list(df.columns)}")
        
        df_clean = df.copy()
        
        # CRITIQUE: V√©rifier et normaliser les noms de colonnes
        # Le fichier original a: edition, round, score, team1, team2, url, venue, year
        column_mapping = {}
        
        # Chercher les colonnes d'√©quipes
        team1_cols = [col for col in df_clean.columns if 'team1' in col.lower() or 'home' in col.lower()]
        team2_cols = [col for col in df_clean.columns if 'team2' in col.lower() or 'away' in col.lower()]
        
        column_mapping['team1'] = team1_cols[0] if team1_cols else df_clean.columns[3]  # team1 est normalement en position 3
        column_mapping['team2'] = team2_cols[0] if team2_cols else df_clean.columns[4]  # team2 est normalement en position 4
        
        logger.debug(f"Mapping colonnes d√©tect√©: {column_mapping}")
        
        # Parser score avec gestion des erreurs
        logger.debug("Parsing des scores...")
        scores = df_clean['score'].apply(self.parse_score)
        df_clean['home_result'] = scores.apply(lambda x: x[0])
        df_clean['away_result'] = scores.apply(lambda x: x[1])
        
        # V√©rifier les scores nuls ou n√©gatifs
        null_scores = ((df_clean['home_result'] == 0) & (df_clean['away_result'] == 0)).sum()
        if null_scores > 0:
            logger.warning(f"‚ö†Ô∏è  {null_scores} matchs avec score 0-0 ou non parsable")
        
        # Normaliser √©quipes - CRITIQUE: cr√©er home_team et away_team
        df_clean['home_team'] = df_clean[column_mapping['team1']].apply(self.normalize_team)
        df_clean['away_team'] = df_clean[column_mapping['team2']].apply(self.normalize_team)
        
        # V√©rifier que les colonnes sont cr√©√©es
        logger.debug(f"Colonnes apr√®s cr√©ation: {list(df_clean.columns)}")
        
        # Normaliser ville
        venue_cols = [col for col in df_clean.columns if 'venue' in col.lower() or 'city' in col.lower()]
        venue_col = venue_cols[0] if venue_cols else df_clean.columns[6]  # venue est normalement en position 6
        
        df_clean['city'] = df_clean[venue_col].apply(self.normalize_city)
        
        # Edition (extraire ann√©e)
        edition_cols = [col for col in df_clean.columns if 'edition' in col.lower()]
        if edition_cols:
            df_clean['edition'] = df_clean[edition_cols[0]].astype(str).str.split('-').str[0]
        else:
            # Fallback: utiliser l'ann√©e
            year_cols = [col for col in df_clean.columns if 'year' in col.lower()]
            if year_cols:
                df_clean['edition'] = df_clean[year_cols[0]].astype(str)
            else:
                logger.warning("‚ö†Ô∏è  Colonne 'edition' ou 'year' non trouv√©e")
                df_clean['edition'] = 'Unknown'
        
        # Round
        round_cols = [col for col in df_clean.columns if 'round' in col.lower()]
        round_col = round_cols[0] if round_cols else df_clean.columns[1]  # round est normalement en position 1
        
        df_clean['round'] = df_clean[round_col].apply(self.normalize_round)
        
        # Result
        df_clean['result'] = df_clean.apply(
            lambda row: self.compute_result(
                row['home_result'], 
                row['away_result'],
                row['home_team'],
                row['away_team']
            ), 
            axis=1
)
        
        # Date: ‚ö†Ô∏è Probl√®me - seulement ann√©e disponible
        # Solution temporaire: utiliser 1er juillet de chaque ann√©e
        year_cols = [col for col in df_clean.columns if 'year' in col.lower()]
        if year_cols:
            df_clean['date'] = df_clean[year_cols[0]].apply(
                lambda y: f"{y}-07-01" if pd.notna(y) else None
            )
            df_clean['date'] = pd.to_datetime(df_clean['date'], errors='coerce')
        else:
            logger.error("‚ùå Colonne 'year' non trouv√©e pour la date")
            df_clean['date'] = None
        
        logger.warning("‚ö†Ô∏è  Dates approximatives (1er juillet) - √Ä enrichir ult√©rieurement")
        
        # V√âRIFICATION CRITIQUE: s'assurer que toutes les colonnes n√©cessaires existent
        required_cols = ['home_team', 'away_team', 'home_result', 'away_result',
                        'result', 'date', 'round', 'city', 'edition']
        
        missing_cols = []
        for col in required_cols:
            if col not in df_clean.columns:
                missing_cols.append(col)
                logger.error(f"‚ùå Colonne manquante: {col}")
        
        if missing_cols:
            raise ValueError(f"Colonnes manquantes apr√®s transformation: {missing_cols}")
        
        # S√©lection colonnes finales
        result_df = df_clean[required_cols].copy()
        
        # Afficher un aper√ßu pour d√©bogage
        logger.info(f"‚úÖ Source 1 transform√©e: {len(result_df)} matchs")
        logger.debug(f"Aper√ßu des premi√®res lignes:\n{result_df.head(3)}")
        logger.debug(f"Types de donn√©es:\n{result_df.dtypes}")
        
        return result_df
    
    def transform_source2(self, df):
        """
        Transformation WorldCupMatches2014.csv
        """
        logger.info("üîÑ Transformation Source 2 (2014)...")
        logger.debug(f"Colonnes re√ßues: {list(df.columns)}")
        logger.debug(f"Shape: {df.shape}")
        
        if df.shape[1] <= 1:
            logger.error("‚ùå Le DataFrame n'a qu'une seule colonne")
            logger.error(f"Contenu de la colonne unique: {df.iloc[:, 0].head(5).tolist()}")
            raise ValueError("Fichier CSV mal format√© - toutes les donn√©es dans une colonne")
        
        df_clean = df.copy()
        
        # Normaliser les noms de colonnes
        df_clean.columns = [col.strip().replace('"', '').replace(';', '') for col in df_clean.columns]
        logger.info(f"üîß Colonnes apr√®s nettoyage: {list(df_clean.columns)}")
        
        # Afficher les premi√®res lignes pour d√©bogage
        logger.debug(f"Aper√ßu des donn√©es:\n{df_clean.head(3)}")
        
        # Chercher les colonnes n√©cessaires avec des patterns flexibles
        column_mapping = {}
        
        # Fonction pour trouver une colonne par patterns
        def find_column(patterns, default_position=None):
            for pattern in patterns:
                for col in df_clean.columns:
                    if pattern in col.lower():
                        return col
            # Si pas trouv√© et position par d√©faut sp√©cifi√©e
            if default_position is not None and len(df_clean.columns) > default_position:
                return df_clean.columns[default_position]
            return None
        
        # Mapping des colonnes
        column_mapping['home_team'] = find_column(['home team', 'hometeam', 'team1'], 5)
        column_mapping['away_team'] = find_column(['away team', 'awayteam', 'team2'], 8)
        column_mapping['home_goals'] = find_column(['home goal', 'homegoals', 'home team goal'], 6)
        column_mapping['away_goals'] = find_column(['away goal', 'awaygoals', 'away team goal'], 7)
        column_mapping['city'] = find_column(['city', 'venue', 'stadium city'], 4)
        column_mapping['stage'] = find_column(['stage', 'round', 'phase'], 2)
        column_mapping['year'] = find_column(['year', 'edition', 'tournament'], 0)
        column_mapping['datetime'] = find_column(['datetime', 'date', 'match date', 'time'], 1)
        
        logger.info(f"üîç Mapping d√©tect√©:")
        for key, value in column_mapping.items():
            logger.info(f"  {key}: {value}")
        
        # CRITIQUE: V√©rifier que les colonnes critiques existent
        required_sources = ['home_team', 'away_team', 'home_goals', 'away_goals']
        missing_sources = [key for key in required_sources if column_mapping[key] is None]
        
        if missing_sources:
            logger.error(f"‚ùå Colonnes sources manquantes: {missing_sources}")
            logger.error(f"Colonnes disponibles: {list(df_clean.columns)}")
            raise ValueError(f"Colonnes sources manquantes: {missing_sources}")
        
        # 1. Cr√©er les colonnes standardis√©es d'√©quipes
        df_clean['home_team'] = df_clean[column_mapping['home_team']].apply(self.normalize_team)
        df_clean['away_team'] = df_clean[column_mapping['away_team']].apply(self.normalize_team)
        
        # 2. Cr√©er les colonnes de scores
        df_clean['home_result'] = pd.to_numeric(
            df_clean[column_mapping['home_goals']], 
            errors='coerce'
        ).fillna(0).astype(int).clip(lower=0)
        
        df_clean['away_result'] = pd.to_numeric(
            df_clean[column_mapping['away_goals']], 
            errors='coerce'
        ).fillna(0).astype(int).clip(lower=0)
        
        logger.info(f"üìä Scores transform√©s: {len(df_clean)} lignes")
        logger.debug(f"Exemple de scores: {df_clean[['home_team', 'home_result', 'away_result', 'away_team']].head(3).to_string()}")
        
        # 3. Cr√©er la colonne result
        df_clean['result'] = df_clean.apply(
            lambda row: self.compute_result(
                row['home_result'], 
                row['away_result'],
                row['home_team'],
                row['away_team']
            ), 
            axis=1
        )
                
        # 4. Cr√©er la colonne city
        if column_mapping['city']:
            df_clean['city'] = df_clean[column_mapping['city']].apply(self.normalize_city)
        else:
            logger.warning("‚ö†Ô∏è  Colonne 'city' non trouv√©e, utilisation de 'Unknown'")
            df_clean['city'] = 'Unknown'
        
        # 5. Cr√©er la colonne round
        if column_mapping['stage']:
            df_clean['round'] = df_clean[column_mapping['stage']].apply(self.normalize_round)
        else:
            logger.warning("‚ö†Ô∏è  Colonne 'stage' non trouv√©e, utilisation de 'Group Stage' par d√©faut")
            df_clean['round'] = 'Group Stage'
        
        # 6. Cr√©er la colonne edition (ann√©e)
        if column_mapping['year']:
            df_clean['edition'] = pd.to_numeric(
                df_clean[column_mapping['year']], 
                errors='coerce'
            ).fillna(0).astype(int).astype(str)
            # Remplacer '0' par 'Unknown'
            df_clean['edition'] = df_clean['edition'].replace('0', 'Unknown')
        else:
            logger.warning("‚ö†Ô∏è  Colonne 'year' non trouv√©e, utilisation de 'Unknown'")
            df_clean['edition'] = 'Unknown'
        
        # 7. Cr√©er la colonne date
        if column_mapping['datetime']:
            df_clean['date'] = df_clean[column_mapping['datetime']].apply(self.parse_datetime)
            # Si certaines dates sont NULL, essayer de les compl√©ter
            null_dates = df_clean['date'].isnull().sum()
            if null_dates > 0:
                logger.warning(f"‚ö†Ô∏è  {null_dates} dates non parsables, tentative de compl√©tion...")
                # Utiliser l'ann√©e + 1er juillet comme fallback
                for idx, row in df_clean[df_clean['date'].isnull()].iterrows():
                    if row['edition'] != 'Unknown' and row['edition'].isdigit():
                        df_clean.at[idx, 'date'] = pd.to_datetime(f"{row['edition']}-07-01")
        else:
            logger.warning("‚ö†Ô∏è  Colonne 'datetime' non trouv√©e, cr√©ation √† partir de l'√©dition")
            df_clean['date'] = df_clean['edition'].apply(
                lambda x: pd.to_datetime(f"{x}-07-01") if x != 'Unknown' and x.isdigit() else pd.NaT
            )
        
        # V√©rifier les dates invalides
        invalid_dates = df_clean['date'].isnull().sum()
        if invalid_dates > 0:
            logger.warning(f"‚ö†Ô∏è  {invalid_dates} dates invalides apr√®s traitement")
            # Remplacer par une date par d√©faut pour √©viter les erreurs
            default_date = pd.to_datetime('1900-01-01')
            df_clean['date'] = df_clean['date'].fillna(default_date)
        
        # V√âRIFICATION FINALE: s'assurer que toutes les colonnes existent
        required_final_cols = ['home_team', 'away_team', 'home_result', 'away_result',
                            'result', 'date', 'round', 'city', 'edition']
        
        logger.info("üîç V√©rification des colonnes finales...")
        for col in required_final_cols:
            if col not in df_clean.columns:
                logger.error(f"‚ùå Colonne finale manquante: {col}")
            else:
                logger.debug(f"‚úÖ Colonne pr√©sente: {col} (type: {df_clean[col].dtype})")
        
        # S√©lectionner uniquement les colonnes n√©cessaires
        result_df = df_clean[required_final_cols].copy()
        
        # Statistiques de transformation
        logger.info(f"""
    üìä R√âSUM√â TRANSFORMATION SOURCE 2:
    - Matchs transform√©s: {len(result_df)}
    - Scores valides: {(result_df['home_result'] > 0) | (result_df['away_result'] > 0).sum()}
    - √âditions uniques: {result_df['edition'].nunique()}
    - Villes uniques: {result_df['city'].nunique()}
    - Premier match: {result_df.iloc[0]['home_team']} vs {result_df.iloc[0]['away_team']}
    - Dernier match: {result_df.iloc[-1]['home_team']} vs {result_df.iloc[-1]['away_team']}
        """)
        
        logger.debug(f"Aper√ßu final:\n{result_df.head(3).to_string()}")
        
        return result_df
        
    def transform_source3(self, df):
        """
        Transformation Fifa_world_cup_matches.csv
        Format suppos√©: Ann√©e, Tour, √âquipe domicile, √âquipe ext√©rieur, etc.
        """
        logger.info("üîÑ Transformation Source 3 (Fifa_world_cup_matches)...")
        logger.info(f"üìä Shape: {df.shape}")
        logger.info(f"üîß Colonnes: {list(df.columns)}")
        
        if len(df) == 0:
            logger.warning("‚ö†Ô∏è  DataFrame vide")
            return pd.DataFrame()
        
        df_clean = df.copy()
        
        # Normaliser les noms de colonnes pour correspondre au format attendu
        df_clean.columns = [col.lower().strip() for col in df_clean.columns]
        logger.debug(f"Colonnes normalis√©es: {list(df_clean.columns)}")
        
        # Chercher les colonnes correspondantes
        col_mapping = {}
        
        # √âquipes domicile
        home_patterns = ['home', 'team1', 'hometeam', 'home team', 'local', '√©quipe domicile']
        for pattern in home_patterns:
            for col in df_clean.columns:
                if pattern in col:
                    col_mapping['home_team'] = col
                    break
            if 'home_team' in col_mapping:
                break
        
        # √âquipes ext√©rieur
        away_patterns = ['away', 'team2', 'awayteam', 'away team', 'visiteur', '√©quipe ext√©rieur']
        for pattern in away_patterns:
            for col in df_clean.columns:
                if pattern in col:
                    col_mapping['away_team'] = col
                    break
            if 'away_team' in col_mapping:
                break
        
        # R√©sultats
        for col in df_clean.columns:
            if 'homegoals' in col or 'home goals' in col or 'home_score' in col:
                col_mapping['home_goals'] = col
            elif 'awaygoals' in col or 'away goals' in col or 'away_score' in col:
                col_mapping['away_goals'] = col
            elif 'score' in col and 'home' not in col and 'away' not in col:
                # Colonne score combin√©
                col_mapping['combined_score'] = col
        
        # Date et autres
        for col in df_clean.columns:
            if 'date' in col:
                col_mapping['date'] = col
            elif 'year' in col or 'edition' in col:
                col_mapping['year'] = col
            elif 'city' in col or 'venue' in col or 'stadium' in col:
                col_mapping['city'] = col
            elif 'round' in col or 'stage' in col or 'tour' in col:
                col_mapping['round'] = col
        
        logger.info(f"üîç Mapping d√©tect√© pour source 3: {col_mapping}")
        
        # Cr√©er le DataFrame final avec les colonnes standardis√©es
        result_df = pd.DataFrame()
        
        # 1. √âquipes
        if 'home_team' in col_mapping:
            result_df['home_team'] = df_clean[col_mapping['home_team']].apply(self.normalize_team)
        else:
            logger.error("‚ùå Colonne home_team non trouv√©e")
            return pd.DataFrame()
        
        if 'away_team' in col_mapping:
            result_df['away_team'] = df_clean[col_mapping['away_team']].apply(self.normalize_team)
        else:
            logger.error("‚ùå Colonne away_team non trouv√©e")
            return pd.DataFrame()
        
        # 2. Scores - g√©rer diff√©rents formats
        if 'home_goals' in col_mapping and 'away_goals' in col_mapping:
            # Format s√©par√©
            result_df['home_result'] = pd.to_numeric(df_clean[col_mapping['home_goals']], errors='coerce').fillna(0).astype(int)
            result_df['away_result'] = pd.to_numeric(df_clean[col_mapping['away_goals']], errors='coerce').fillna(0).astype(int)
        elif 'combined_score' in col_mapping:
            # Format combin√© "X-Y"
            scores = df_clean[col_mapping['combined_score']].apply(self.parse_score)
            result_df['home_result'] = scores.apply(lambda x: x[0])
            result_df['away_result'] = scores.apply(lambda x: x[1])
        else:
            logger.warning("‚ö†Ô∏è  Aucune colonne de score trouv√©e, utilisation de scores par d√©faut")
            result_df['home_result'] = 0
            result_df['away_result'] = 0
        
        # 3. R√©sultat AVEC NOM DU GAGNANT
        result_df['result'] = result_df.apply(
            lambda row: self.compute_result(
                row['home_result'], 
                row['away_result'],
                row['home_team'],
                row['away_team']
            ),
            axis=1
        )
        # 4. Date
        if 'date' in col_mapping:
            result_df['date'] = df_clean[col_mapping['date']].apply(self.parse_datetime)
        else:
            logger.warning("‚ö†Ô∏è  Colonne date non trouv√©e, tentative avec ann√©e")
            result_df['date'] = None
        
        # 5. √âdition/Ann√©e
        if 'year' in col_mapping:
            result_df['edition'] = df_clean[col_mapping['year']].astype(str)
            # Compl√©ter les dates manquantes avec l'ann√©e
            if result_df['date'].isnull().any() and 'edition' in result_df.columns:
                for idx, row in result_df[result_df['date'].isnull()].iterrows():
                    if row['edition'].isdigit():
                        result_df.at[idx, 'date'] = pd.to_datetime(f"{row['edition']}-07-01")
        else:
            logger.warning("‚ö†Ô∏è  Colonne edition/year non trouv√©e")
            result_df['edition'] = 'Unknown'
        
        # 6. Ville
        if 'city' in col_mapping:
            result_df['city'] = df_clean[col_mapping['city']].apply(self.normalize_city)
        else:
            result_df['city'] = 'Unknown'
        
        # 7. Round
        if 'round' in col_mapping:
            result_df['round'] = df_clean[col_mapping['round']].apply(self.normalize_round)
        else:
            result_df['round'] = 'Group Stage'
        
        # V√©rification finale
        logger.info(f"üìä Source 3 transform√©e: {len(result_df)} matchs")
        
        # G√©rer les dates manquantes
        if result_df['date'].isnull().any():
            logger.warning(f"‚ö†Ô∏è  {result_df['date'].isnull().sum()} dates manquantes apr√®s traitement")
            # Remplacer par date par d√©faut
            default_date = pd.to_datetime('1900-01-01')
            result_df['date'] = result_df['date'].fillna(default_date)
        
        logger.debug(f"Aper√ßu source 3:\n{result_df.head(3)}")
        
        return result_df
    
    def transform_source4(self, json_data):
        """
        Transformation data_2018.json
        """
        logger.info("üîÑ Transformation Source 4 (Coupe du Monde 2018)...")
        
        if not json_data:
            logger.warning("‚ö†Ô∏è  Donn√©es JSON vides")
            return pd.DataFrame()
        
        # Extraire les matchs des groupes
        group_matches = []
        for group_name, group_data in json_data.get('groups', {}).items():
            for match in group_data.get('matches', []):
                match_data = match.copy()
                match_data['type'] = 'group'
                match_data['group'] = group_name
                group_matches.append(match_data)
        
        # Extraire les matchs √† √©limination directe
        knockout_matches = []
        knockout_rounds = json_data.get('knockout', {})
        
        # Round of 16
        for match in knockout_rounds.get('round_16', {}).get('matches', []):
            match_data = match.copy()
            match_data['type'] = 'knockout'
            match_data['round'] = 'Round of 16'
            knockout_matches.append(match_data)
        
        # Quarter-finals
        for match in knockout_rounds.get('round_8', {}).get('matches', []):
            match_data = match.copy()
            match_data['type'] = 'knockout'
            match_data['round'] = 'Quarter-finals'
            knockout_matches.append(match_data)
        
        # Semi-finals
        for match in knockout_rounds.get('round_4', {}).get('matches', []):
            match_data = match.copy()
            match_data['type'] = 'knockout'
            match_data['round'] = 'Semi-finals'
            knockout_matches.append(match_data)
        
        # Third Place
        for match in knockout_rounds.get('round_2_loser', {}).get('matches', []):
            match_data = match.copy()
            match_data['type'] = 'knockout'
            match_data['round'] = 'Third Place'
            knockout_matches.append(match_data)
        
        # Final
        for match in knockout_rounds.get('round_2', {}).get('matches', []):
            match_data = match.copy()
            match_data['type'] = 'knockout'
            match_data['round'] = 'Final'
            knockout_matches.append(match_data)
        
        # Combiner tous les matchs
        all_matches = group_matches + knockout_matches
        
        logger.info(f"üìä Total matchs extraits: {len(all_matches)}")
        logger.info(f"  - Matchs de groupe: {len(group_matches)}")
        logger.info(f"  - Matchs √† √©limination: {len(knockout_matches)}")
        
        if len(all_matches) == 0:
            logger.warning("‚ö†Ô∏è  Aucun match trouv√© dans les donn√©es")
            return pd.DataFrame()
        
        # Cr√©er le DataFrame
        matches_list = []
        
        for match in all_matches:
            # Convertir les IDs d'√©quipes en noms
            home_team_id = match.get('home_team')
            away_team_id = match.get('away_team')
            
            if home_team_id in self.teams_mapping_2018:
                home_team = self.teams_mapping_2018[home_team_id]
            else:
                logger.warning(f"‚ö†Ô∏è  ID √©quipe domicile inconnu: {home_team_id}")
                home_team = f"Unknown_{home_team_id}"
            
            if away_team_id in self.teams_mapping_2018:
                away_team = self.teams_mapping_2018[away_team_id]
            else:
                logger.warning(f"‚ö†Ô∏è  ID √©quipe ext√©rieur inconnu: {away_team_id}")
                away_team = f"Unknown_{away_team_id}"
            
            # Normaliser les noms d'√©quipes avec le mapping historique
            home_team_norm = self.normalize_team(home_team)
            away_team_norm = self.normalize_team(away_team)
            
            # Extraire le score
            home_result = match.get('home_result', 0)
            away_result = match.get('away_result', 0)
            
           # Calculer le r√©sultat AVEC NOM DU GAGNANT
            result = self.compute_result(home_result, away_result, home_team_norm, away_team_norm)
            
            # G√©rer la date
            date_str = match.get('date')
            if date_str:
                # Extraire la partie date seulement (avant 'T')
                date_part = date_str.split('T')[0]
                try:
                    date_obj = pd.to_datetime(date_part)
                except:
                    date_obj = pd.to_datetime('2018-07-01')  # Date par d√©faut
            else:
                date_obj = pd.to_datetime('2018-07-01')
            
            # G√©rer la ville/stade
            stadium_id = match.get('stadium')
            city = "Unknown"
            
            # Chercher la ville dans les donn√©es des stades
            if 'stadiums' in json_data:
                for stadium in json_data['stadiums']:
                    if stadium.get('id') == stadium_id:
                        city = stadium.get('city', 'Unknown')
                        break
            
            # Normaliser la ville
            city_norm = self.normalize_city(city)
            
            # G√©rer le round
            round_str = match.get('round')
            if not round_str:
                round_str = 'Group Stage' if match.get('type') == 'group' else 'Knockout'
            
            round_norm = self.normalize_round(round_str)
            
            # Ajouter au DataFrame
            match_dict = {
                'home_team': home_team_norm,
                'away_team': away_team_norm,
                'home_result': home_result,
                'away_result': away_result,
                'result': result,
                'date': date_obj,
                'round': round_norm,
                'city': city_norm,
                'edition': '2018',
                'source': 'json_2018',
                'match_id_2018': match.get('name'),
                'stadium_id': stadium_id
            }
            
            matches_list.append(match_dict)
        
        # Cr√©er DataFrame
        df_clean = pd.DataFrame(matches_list)
        
        # Statistiques
        logger.info(f"""
    üìä R√âSUM√â TRANSFORMATION SOURCE 4 (2018):
    - Matchs transform√©s: {len(df_clean)}
    - √âquipes uniques: {pd.concat([df_clean['home_team'], df_clean['away_team']]).nunique()}
    - Villes uniques: {df_clean['city'].nunique()}
    - Rounds uniques: {df_clean['round'].unique()}
    - P√©riode: {df_clean['date'].min().date()} au {df_clean['date'].max().date()}
        """)
        
        # Afficher quelques exemples
        logger.debug(f"Aper√ßu des matchs 2018:\n{df_clean.head(3).to_string()}")
        
        # V√©rification de base
        if len(df_clean) != 64:
            logger.warning(f"‚ö†Ô∏è  Attendu 64 matchs pour 2018, obtenu {len(df_clean)}")
        
        return df_clean
    
    def enrich_with_stadiums(self, df_2018, json_data):
        """
        Enrichir les donn√©es avec les informations des stades
        """
        logger.info("üèüÔ∏è  Enrichissement avec les donn√©es des stades...")
        
        if 'stadiums' not in json_data or len(json_data['stadiums']) == 0:
            logger.warning("‚ö†Ô∏è  Aucune donn√©e de stade disponible")
            return df_2018
        
        # Cr√©er un DataFrame des stades
        stadiums_df = pd.DataFrame(json_data['stadiums'])
        
        # V√©rifier les colonnes disponibles
        logger.info(f"Colonnes stades disponibles: {list(stadiums_df.columns)}")
        
        # Renommer pour correspondre
        stadiums_df = stadiums_df.rename(columns={
            'id': 'stadium_id',
            'name': 'stadium_name'
        })
        
        # Fusionner avec les matchs (si stadium_id existe)
        if 'stadium_id' in df_2018.columns:
            df_enriched = pd.merge(
                df_2018,
                stadiums_df[['stadium_id', 'stadium_name', 'lat', 'lng']],
                on='stadium_id',
                how='left'
            )
            
            logger.info(f"‚úÖ {df_enriched['stadium_name'].notnull().sum()} matchs enrichis avec donn√©es stade")
            return df_enriched
        else:
            logger.warning("‚ö†Ô∏è  Colonne stadium_id non trouv√©e dans les donn√©es 2018")
            return df_2018
    
    def enrich_with_teams_info(self, df_2018, json_data):
        """
        Enrichir avec les informations des √©quipes (drapeaux, codes FIFA)
        """
        logger.info("üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø Enrichissement avec les donn√©es des √©quipes...")
        
        if 'teams' not in json_data or len(json_data['teams']) == 0:
            logger.warning("‚ö†Ô∏è  Aucune donn√©e d'√©quipe disponible")
            return df_2018
        
        # Cr√©er un DataFrame des √©quipes
        teams_df = pd.DataFrame(json_data['teams'])
        
        # Ajouter les informations des √©quipes pour home_team
        df_enriched = df_2018.copy()
        
        # Pour chaque √©quipe, essayer de trouver des informations
        for team_type in ['home', 'away']:
            team_col = f'{team_type}_team'
            
            # Cr√©er des colonnes pour les informations d'√©quipe
            for info_col in ['fifaCode', 'iso2', 'flag']:
                new_col = f'{team_type}_{info_col.lower()}'
                df_enriched[new_col] = None
        
        logger.info(f"‚úÖ Informations d'√©quipe disponibles pour {len(teams_df)} √©quipes")
        
        # Note: l'enrichissement complet n√©cessiterait un mapping nom d'√©quipe -> ID
        # Pour l'instant, on se contente de marquer que l'information est disponible
        
        return df_enriched
    def consolidate(self, dfs_list):
        """
        Consolide toutes les sources et g√©n√®re id_match s√©quentiel
        """
        logger.info("üîó Consolidation des sources...")
        
        # Filtrer les None
        dfs_list = [df for df in dfs_list if df is not None]
        
        if not dfs_list:
            raise ValueError("Aucune donn√©e √† consolider")
        
        logger.info(f"üìä {len(dfs_list)} sources √† consolider")
        for i, df in enumerate(dfs_list, 1):
            logger.info(f"  Source {i}: {len(df)} matchs")
        
        # Concat√©nation
        df_all = pd.concat(dfs_list, ignore_index=True)
        
        logger.info(f"üìä Total avant nettoyage: {len(df_all)} matchs")
        
        # Supprimer les lignes avec valeurs manquantes critiques
        initial_count = len(df_all)
        df_all = df_all.dropna(subset=['home_team', 'away_team', 'date'])
        logger.info(f"üìä {initial_count - len(df_all)} lignes supprim√©es (valeurs manquantes)")
        
        # Supprimer doublons (m√™me √©quipes + m√™me date)
        initial_count = len(df_all)
        df_all = df_all.drop_duplicates(
            subset=['home_team', 'away_team', 'date'],
            keep='first'
        )
        logger.info(f"üìä {initial_count - len(df_all)} doublons supprim√©s")
        
        logger.info(f"üìä Total apr√®s nettoyage: {len(df_all)} matchs")
        
        # Trier chronologiquement
        df_all['date'] = pd.to_datetime(df_all['date'])
        df_all = df_all.sort_values('date').reset_index(drop=True)
        
        # G√©n√©rer id_match s√©quentiel (1 = premier match historique)
        df_all['id_match'] = range(1, len(df_all) + 1)
        
        # R√©organiser colonnes dans l'ordre final
        df_final = df_all[[
            'id_match', 'home_team', 'away_team', 'home_result', 'away_result',
            'result', 'date', 'round', 'city', 'edition'
        ]].copy()
        
        logger.info("‚úÖ Consolidation termin√©e")
        return df_final
    
    def validate(self, df):
        """
        Validation finale des donn√©es
        """
        logger.info("‚úîÔ∏è  Validation des donn√©es...")
        
        issues = []
        
        # V√©rifier colonnes obligatoires
        required_cols = ['id_match', 'home_team', 'away_team', 'home_result', 
                        'away_result', 'result', 'date', 'round', 'city', 'edition']
        
        missing_cols = set(required_cols) - set(df.columns)
        if missing_cols:
            issues.append(f"Colonnes manquantes: {missing_cols}")
        
        # V√©rifier NULL
        null_counts = df[required_cols].isnull().sum()
        null_issues = null_counts[null_counts > 0]
        if not null_issues.empty:
            issues.append(f"Valeurs NULL d√©tect√©es:\n{null_issues}")
        
        # V√©rifier result values
        # V√©rifier result values
        # Maintenant result contient soit:
        # - Le nom d'une √©quipe (gagnant)
        # - "draw" (match nul)
        # - None (erreur)

        # V√©rifier qu'il n'y a pas de valeurs comme "home_team" ou "away_team"
        invalid_values = df[df['result'].isin(['home_team', 'away_team'])]
        if len(invalid_values) > 0:
            issues.append(f"Valeurs 'result' invalides (home_team/away_team): {len(invalid_values)} lignes")
            logger.warning(f"‚ö†Ô∏è  Lignes avec result invalide:\n{invalid_values[['home_team', 'away_team', 'result']].head()}")

        # V√©rifier que les matchs nuls sont bien marqu√©s "draw"
        draw_matches = df[df['result'] == 'draw']
        draw_but_not_equal = draw_matches[draw_matches['home_result'] != draw_matches['away_result']]
        if len(draw_but_not_equal) > 0:
            issues.append(f"{len(draw_but_not_equal)} matchs marqu√©s 'draw' mais avec scores diff√©rents")

        # V√©rifier que les gagnants correspondent aux scores
        for idx, row in df.iterrows():
            if row['result'] != 'draw' and row['result'] is not None:
                # V√©rifier que le gagnant correspond au score
                if row['result'] == row['home_team'] and row['home_result'] <= row['away_result']:
                    issues.append(f"Ligne {idx}: {row['home_team']} marqu√© gagnant mais {row['home_result']} ‚â§ {row['away_result']}")
                elif row['result'] == row['away_team'] and row['away_result'] <= row['home_result']:
                    issues.append(f"Ligne {idx}: {row['away_team']} marqu√© gagnant mais {row['away_result']} ‚â§ {row['home_result']}")
        
        # V√©rifier id_match s√©quentiel
        expected_ids = set(range(1, len(df) + 1))
        actual_ids = set(df['id_match'])
        if expected_ids != actual_ids:
            issues.append("id_match non s√©quentiel ou avec gaps")
        
        # V√©rifier doublons
        duplicates = df.duplicated(subset=['home_team', 'away_team', 'date']).sum()
        if duplicates > 0:
            issues.append(f"{duplicates} doublons d√©tect√©s")
        
        # Rapport
        if issues:
            logger.warning("‚ö†Ô∏è  Probl√®mes de validation d√©tect√©s:")
            for issue in issues:
                logger.warning(f"  - {issue}")
            return False
        else:
            logger.info("‚úÖ Validation r√©ussie - Toutes les v√©rifications pass√©es")
            return True
        
        # Stats (toujours afficher)
        logger.info(f"""
üìä Statistiques finales:
  - Total matchs: {len(df)}
  - √âditions: {df['edition'].nunique()} ({df['edition'].min()} - {df['edition'].max()})
  - √âquipes uniques: {pd.concat([df['home_team'], df['away_team']]).nunique()}
  - Villes: {df['city'].nunique()}
  - Premier match (id=1): {df.iloc[0]['home_team']} vs {df.iloc[0]['away_team']} ({df.iloc[0]['date'].strftime('%Y-%m-%d')})
  - Dernier match (id={len(df)}): {df.iloc[-1]['home_team']} vs {df.iloc[-1]['away_team']} ({df.iloc[-1]['date'].strftime('%Y-%m-%d')})
        """)
        
        return True

    def analyze_results(self, df):
        """
        Analyse des r√©sultats pour v√©rification
        """
        logger.info("üìä Analyse des r√©sultats...")
        
        total_matches = len(df)
        draws = df[df['result'] == 'draw']
        home_wins = df[df['result'] == df['home_team']]
        away_wins = df[df['result'] == df['away_team']]
        
        logger.info(f"""
    üìà Statistiques des r√©sultats:
    - Total matchs: {total_matches}
    - Victoires √† domicile: {len(home_wins)} ({len(home_wins)/total_matches*100:.1f}%)
    - Victoires √† l'ext√©rieur: {len(away_wins)} ({len(away_wins)/total_matches*100:.1f}%)
    - Matchs nuls: {len(draws)} ({len(draws)/total_matches*100:.1f}%)
    
    üéØ Exemples de r√©sultats:
    - Victoire domicile: {home_wins.iloc[0]['home_team']} {home_wins.iloc[0]['home_result']}-{home_wins.iloc[0]['away_result']} {home_wins.iloc[0]['away_team']} ‚Üí {home_wins.iloc[0]['result']}
    - Victoire ext√©rieur: {away_wins.iloc[0]['home_team']} {away_wins.iloc[0]['home_result']}-{away_wins.iloc[0]['away_result']} {away_wins.iloc[0]['away_team']} ‚Üí {away_wins.iloc[0]['result']}
    - Match nul: {draws.iloc[0]['home_team']} {draws.iloc[0]['home_result']}-{draws.iloc[0]['away_result']} {draws.iloc[0]['away_team']} ‚Üí {draws.iloc[0]['result']}
        """)
        
        # V√©rifier les valeurs probl√©matiques
        problematic = df[(df['result'] != 'draw') & 
                        (df['result'] != df['home_team']) & 
                        (df['result'] != df['away_team'])]
        
        if len(problematic) > 0:
            logger.warning(f"‚ö†Ô∏è  {len(problematic)} r√©sultats probl√©matiques:")
            for idx, row in problematic.head(5).iterrows():
                logger.warning(f"  - Ligne {idx}: {row['home_team']} {row['home_result']}-{row['away_result']} {row['away_team']} ‚Üí {row['result']}")
# =====================================================================
# CLASSE LOAD (Data Engineer 3)
# =====================================================================

class WorldCupLoader:
    """Chargement des donn√©es dans SQLite"""
    
    def __init__(self, db_path="data/worldcup.db"):
        self.db_path = db_path
        self.conn = None
    
    def connect(self):
        """Connexion √† la base SQLite"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row  # Pour obtenir les r√©sultats sous forme de dict
            logger.info(f"‚úÖ Connexion √† {self.db_path} √©tablie")
        except Exception as e:
            logger.error(f"‚ùå Erreur connexion DB: {e}")
            raise
    
    def create_schema(self):
        """Cr√©ation du sch√©ma de la table principale"""
        logger.info("üèóÔ∏è  Cr√©ation du sch√©ma principal...")
        
        create_table_sql = """
        -- Table principale des matchs
        DROP TABLE IF EXISTS world_cup_matches;
        
        CREATE TABLE world_cup_matches (
            id_match INTEGER PRIMARY KEY,
            home_team TEXT NOT NULL,
            away_team TEXT NOT NULL,
            home_result INTEGER NOT NULL,
            away_result INTEGER NOT NULL, 
            result TEXT,
            date DATE NOT NULL,
            round TEXT NOT NULL,
            city TEXT NOT NULL,
            edition TEXT NOT NULL,
            -- Nouvelles colonnes optionnelles
            source TEXT,
            match_id_2018 INTEGER,
            stadium_id INTEGER,
            stadium_name TEXT,
            home_fifacode TEXT,
            away_fifacode TEXT
        );
        
        -- Index pour optimiser les requ√™tes
        CREATE INDEX idx_edition ON world_cup_matches(edition);
        CREATE INDEX idx_teams ON world_cup_matches(home_team, away_team);
        CREATE INDEX idx_date ON world_cup_matches(date);
        CREATE INDEX idx_round ON world_cup_matches(round);
        CREATE INDEX idx_city ON world_cup_matches(city);
        CREATE INDEX idx_result ON world_cup_matches(result);
        
        -- Table pour les stades (optionnel)
        DROP TABLE IF EXISTS stadiums;
        
        CREATE TABLE IF NOT EXISTS stadiums (
            stadium_id INTEGER PRIMARY KEY,
            name TEXT NOT NULL,
            city TEXT NOT NULL,
            lat REAL,
            lng REAL,
            image TEXT
        );
        
        -- Table pour les √©quipes (optionnel)
        DROP TABLE IF EXISTS teams;
        
        CREATE TABLE IF NOT EXISTS teams (
            team_id INTEGER PRIMARY KEY,
            name TEXT NOT NULL,
            fifaCode TEXT,
            iso2 TEXT,
            flag TEXT,
            emojiString TEXT
        );
        
        -- Table pour les cha√Ænes TV (optionnel)
        DROP TABLE IF EXISTS tv_channels;
        
        CREATE TABLE IF NOT EXISTS tv_channels (
            channel_id INTEGER PRIMARY KEY,
            name TEXT NOT NULL,
            icon TEXT,
            country TEXT,
            iso2 TEXT,
            languages TEXT
        );
        """
        
        try:
            self.conn.executescript(create_table_sql)
            self.conn.commit()
            logger.info("‚úÖ Sch√©ma principal cr√©√© avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur cr√©ation sch√©ma: {e}")
            raise
    
    def load_data(self, df):
        """Chargement des donn√©es dans la table principale"""
        logger.info(f"üì§ Chargement de {len(df)} matchs dans la table principale...")
        
        try:
            # Pr√©parer les donn√©es pour SQLite
            df_to_load = df.copy()
            
            # Convertir date en string pour SQLite
            df_to_load['date'] = df_to_load['date'].dt.strftime('%Y-%m-%d')
            
            # S'assurer que toutes les colonnes optionnelles existent
            optional_columns = ['source', 'match_id_2018', 'stadium_id', 
                              'stadium_name', 'home_fifacode', 'away_fifacode']
            
            for col in optional_columns:
                if col not in df_to_load.columns:
                    df_to_load[col] = None
            
            # S√©lectionner uniquement les colonnes qui existent dans le sch√©ma
            table_columns = ['id_match', 'home_team', 'away_team', 'home_result', 'away_result',
                           'result', 'date', 'round', 'city', 'edition', 'source',
                           'match_id_2018', 'stadium_id', 'stadium_name',
                           'home_fifacode', 'away_fifacode']
            
            # Garder seulement les colonnes qui existent dans DataFrame
            available_columns = [col for col in table_columns if col in df_to_load.columns]
            df_to_load = df_to_load[available_columns]
            
            # Insert dans SQLite
            df_to_load.to_sql(
                'world_cup_matches',
                self.conn,
                if_exists='append',
                index=False
            )
            
            self.conn.commit()
            logger.info("‚úÖ Donn√©es principales charg√©es avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement donn√©es principales: {e}")
            self.conn.rollback()
            raise
    
    def load_additional_data(self, json_data):
        """
        Charger les donn√©es suppl√©mentaires (stades, √©quipes, cha√Ænes)
        """
        logger.info("üì§ Chargement des donn√©es suppl√©mentaires...")
        
        try:
            # 1. Charger les stades
            if 'stadiums' in json_data and json_data['stadiums']:
                stadiums_df = pd.DataFrame(json_data['stadiums'])
                # Renommer pour correspondre au sch√©ma
                stadiums_df = stadiums_df.rename(columns={'id': 'stadium_id'})
                stadiums_df.to_sql('stadiums', self.conn, if_exists='replace', index=False)
                logger.info(f"‚úÖ {len(stadiums_df)} stades charg√©s")
            
            # 2. Charger les √©quipes
            if 'teams' in json_data and json_data['teams']:
                teams_df = pd.DataFrame(json_data['teams'])
                # Renommer pour correspondre au sch√©ma
                teams_df = teams_df.rename(columns={'id': 'team_id'})
                # Convertir la liste de langues en string
                if 'lang' in teams_df.columns:
                    teams_df['languages'] = teams_df['lang'].apply(lambda x: ','.join(x) if isinstance(x, list) else str(x))
                    teams_df = teams_df.drop('lang', axis=1)
                teams_df.to_sql('teams', self.conn, if_exists='replace', index=False)
                logger.info(f"‚úÖ {len(teams_df)} √©quipes charg√©es")
            
            # 3. Charger les cha√Ænes TV
            if 'tvchannels' in json_data and json_data['tvchannels']:
                channels_df = pd.DataFrame(json_data['tvchannels'])
                # Renommer pour correspondre au sch√©ma
                channels_df = channels_df.rename(columns={'id': 'channel_id'})
                # Convertir la liste de langues en string
                if 'lang' in channels_df.columns:
                    channels_df['languages'] = channels_df['lang'].apply(lambda x: ','.join(x) if isinstance(x, list) else str(x))
                    channels_df = channels_df.drop('lang', axis=1)
                channels_df.to_sql('tv_channels', self.conn, if_exists='replace', index=False)
                logger.info(f"‚úÖ {len(channels_df)} cha√Ænes TV charg√©es")
            
            self.conn.commit()
            logger.info("‚úÖ Donn√©es suppl√©mentaires charg√©es avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement donn√©es suppl√©mentaires: {e}")
            self.conn.rollback()
            raise
    
    def verify_load(self):
        """V√©rification du chargement"""
        logger.info("üîç V√©rification du chargement...")
        
        try:
            # 1. V√©rifier la table principale
            query = "SELECT COUNT(*) as total FROM world_cup_matches"
            result = pd.read_sql_query(query, self.conn)
            total_matches = result['total'][0]
            logger.info(f"‚úÖ {total_matches} matchs en base")
            
            # 2. Statistiques par √©dition
            query_editions = """
            SELECT edition, COUNT(*) as matchs
            FROM world_cup_matches
            GROUP BY edition
            ORDER BY edition
            """
            editions_df = pd.read_sql_query(query_editions, self.conn)
            logger.info("üìä R√©partition par √©dition:")
            for _, row in editions_df.iterrows():
                logger.info(f"  - {row['edition']}: {row['matchs']} matchs")
            
            # 3. Premier et dernier match
            query_first = """
            SELECT id_match, home_team, away_team, date, edition 
            FROM world_cup_matches 
            WHERE id_match = 1
            """
            first = pd.read_sql_query(query_first, self.conn)
            if len(first) > 0:
                logger.info(f"ü•á Premier match (id=1): {first.iloc[0]['home_team']} vs {first.iloc[0]['away_team']} ({first.iloc[0]['date']}) - {first.iloc[0]['edition']}")
            
            query_last = """
            SELECT id_match, home_team, away_team, date, edition 
            FROM world_cup_matches 
            ORDER BY id_match DESC 
            LIMIT 1
            """
            last = pd.read_sql_query(query_last, self.conn)
            if len(last) > 0:
                logger.info(f"üèÜ Dernier match (id={last.iloc[0]['id_match']}): {last.iloc[0]['home_team']} vs {last.iloc[0]['away_team']} ({last.iloc[0]['date']}) - {last.iloc[0]['edition']}")
            
            # 4. V√©rifier les tables suppl√©mentaires
            for table in ['stadiums', 'teams', 'tv_channels']:
                try:
                    query = f"SELECT COUNT(*) as count FROM {table}"
                    result = pd.read_sql_query(query, self.conn)
                    if result['count'][0] > 0:
                        logger.info(f"‚úÖ Table {table}: {result['count'][0]} enregistrements")
                    else:
                        logger.warning(f"‚ö†Ô∏è  Table {table}: vide")
                except:
                    logger.debug(f"‚ÑπÔ∏è  Table {table} non trouv√©e (optionnel)")
            
            # 5. Statistiques g√©n√©rales
            query_stats = """
            SELECT 
                COUNT(DISTINCT home_team || away_team) as matchs_uniques,
                COUNT(DISTINCT home_team) as equipes_domicile,
                COUNT(DISTINCT away_team) as equipes_exterieur,
                MIN(date) as date_debut,
                MAX(date) as date_fin
            FROM world_cup_matches
            """
            stats = pd.read_sql_query(query_stats, self.conn)
            if len(stats) > 0:
                logger.info(f"""
üìà Statistiques globales:
  - Matchs uniques: {stats.iloc[0]['matchs_uniques']}
  - √âquipes domicile uniques: {stats.iloc[0]['equipes_domicile']}
  - √âquipes ext√©rieur uniques: {stats.iloc[0]['equipes_exterieur']}
  - P√©riode couverte: {stats.iloc[0]['date_debut']} au {stats.iloc[0]['date_fin']}
                """)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification: {e}")
            raise
    
    def clean_database(self):
        """Nettoyer compl√®tement la base de donn√©es"""
        logger.info("üßπ Nettoyage complet de la base de donn√©es...")
        
        drop_all_tables = """
        DROP TABLE IF EXISTS world_cup_matches;
        DROP TABLE IF EXISTS stadiums;
        DROP TABLE IF EXISTS teams;
        DROP TABLE IF EXISTS tv_channels;
        """
        
        try:
            self.conn.executescript(drop_all_tables)
            self.conn.commit()
            logger.info("‚úÖ Base de donn√©es nettoy√©e")
        except Exception as e:
            logger.error(f"‚ùå Erreur nettoyage base: {e}")
            raise

    def close(self):
        """Fermeture connexion"""
        if self.conn:
            self.conn.close()
            logger.info("‚úÖ Connexion ferm√©e")
    
        """Fermeture connexion"""
        if self.conn:
            self.conn.close()
            logger.info("‚úÖ Connexion ferm√©e")


# =====================================================================
# PIPELINE PRINCIPAL (Chef de Projet)
# =====================================================================

def run_etl_pipeline():
    """
    Pipeline ETL complet
    """
    logger.info("="*70)
    logger.info("üöÄ D√âBUT DU PIPELINE ETL - FIFA WORLD CUP")
    logger.info("="*70)
    
    try:
        # -------------------- EXTRACT --------------------
        logger.info("\nüì• PHASE 1: EXTRACTION")
        logger.info("-" * 50)
        
        extractor = WorldCupExtractor(data_dir="data/raw")
        
        df_source1 = extractor.extract_source1("matches_1930-2010.csv")
        df_source2 = extractor.extract_source2("WorldCupMatches2014.csv")
        df_source3 = extractor.extract_source3("Fifa_world_cup_matches.csv")
        json_source4 = extractor.extract_source4("data_2018.json")
        
        # -------------------- TRANSFORM --------------------
        logger.info("\nüîÑ PHASE 2: TRANSFORMATION")
        logger.info("-" * 50)
        
        transformer = WorldCupTransformer()
        
        df_clean1 = transformer.transform_source1(df_source1)
        df_clean2 = transformer.transform_source2(df_source2)
        df_clean3 = transformer.transform_source3(df_source3)
        df_clean4 = transformer.transform_source4(json_source4)
        
        # Enrichissement optionnel des donn√©es 2018
        if len(df_clean4) > 0:
            logger.info("\nüåü Enrichissement des donn√©es 2018...")
            df_clean4_enriched = transformer.enrich_with_stadiums(df_clean4, json_source4)
            df_clean4_enriched = transformer.enrich_with_teams_info(df_clean4_enriched, json_source4)
            
            # Sauvegarder les donn√©es enrichies s√©par√©ment
            df_clean4_enriched.to_csv("data/processed/worldcup_2018_enriched.csv", index=False)
            logger.info("üíæ Donn√©es 2018 enrichies sauvegard√©es: data/processed/worldcup_2018_enriched.csv")
            
            # Utiliser les donn√©es enrichies pour la consolidation
            df_clean4 = df_clean4_enriched
        
        # Consolidation avec les 4 sources
        df_final = transformer.consolidate([df_clean1, df_clean2, df_clean3, df_clean4])
        
        # Analyse des r√©sultats (NOUVEAU)
        transformer.analyze_results(df_final)
        # Validation
        is_valid = transformer.validate(df_final)
        
        if not is_valid:
            logger.warning("‚ö†Ô∏è  Des probl√®mes de validation ont √©t√© d√©tect√©s, mais le pipeline continue")
        
        # -------------------- LOAD --------------------
        logger.info("\nüì§ PHASE 3: CHARGEMENT")
        logger.info("-" * 50)
        
        loader = WorldCupLoader(db_path="data/worldcup.db")
        loader.connect()
        # Nettoyer la base de donn√©es
        loader.clean_database()
        loader.create_schema()
        
        # 1. Charger les donn√©es principales
        loader.load_data(df_final)
        
        # 2. Charger les donn√©es suppl√©mentaires (stades, √©quipes, cha√Ænes)
        loader.load_additional_data(json_source4)
        
        # 3. V√©rifier le chargement
        loader.verify_load()
        loader.close()
        
        # -------------------- SUCC√àS --------------------
        logger.info("\n" + "="*70)
        logger.info("‚úÖ PIPELINE ETL TERMIN√â AVEC SUCC√àS")
        logger.info("="*70)
        logger.info(f"""
üìä R√©sum√©:
  - Sources trait√©es: 4/4 (CSV 1930-2010 + CSV 2014 + CSV FIFA + JSON 2018)
  - Total matchs charg√©s: {len(df_final)}
  - Base de donn√©es: data/worldcup.db
  - P√©riode couverte: {df_final['date'].min().year} - {df_final['date'].max().year}
  - √âquipes uniques: {pd.concat([df_final['home_team'], df_final['away_team']]).nunique()}
  - Tables cr√©√©es: world_cup_matches, stadiums, teams, tv_channels
  
üéØ Points forts:
  ‚úì Donn√©es historiques compl√®tes (1930-2018)
  ‚úì √âdition 2018 avec stades et √©quipes d√©taill√©s
  ‚úì Sch√©ma relationnel complet
  ‚úì Validation rigoureuse
  ‚úì Index pour performances
  
üìà Acc√®s aux donn√©es:
  - Fichier CSV: data/processed/worldcup_clean.csv
  - Base SQLite: data/worldcup.db
  - Donn√©es 2018 enrichies: data/processed/worldcup_2018_enriched.csv
  
üîç Exemples de requ√™tes SQL:
  1. SELECT * FROM world_cup_matches WHERE edition = '2018';
  2. SELECT round, COUNT(*) FROM world_cup_matches GROUP BY round;
  3. SELECT * FROM stadiums WHERE city = 'Moscow';
  4. SELECT team.* FROM teams team JOIN world_cup_matches m ON team.name = m.home_team;
        """)
        
        return df_final
        
    except Exception as e:
        logger.error(f"\n‚ùå ERREUR PIPELINE: {e}")
        import traceback
        traceback.print_exc()
   


# =====================================================================
# POINT D'ENTR√âE
# =====================================================================

if __name__ == "__main__":
    # Cr√©er r√©pertoires si n√©cessaire
    Path("data/raw").mkdir(parents=True, exist_ok=True)
    Path("data/processed").mkdir(parents=True, exist_ok=True)
    
    # Lancer le pipeline
    df_result = run_etl_pipeline()
    
    # Sauvegarder aussi en CSV pour inspection
    df_result.to_csv("data/processed/worldcup_clean.csv", index=False)
    logger.info("üíæ Donn√©es sauvegard√©es aussi en CSV: data/processed/worldcup_clean.csv")